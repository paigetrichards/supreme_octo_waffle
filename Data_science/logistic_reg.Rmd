---
title: "Logistic Regression"
subtitle: "Data Science for Biologists, Spring 2020"
author: "Paige Richards"
output: 
  html_document:
    theme: readable
    highlight: espresso
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom) 
library(pROC)
library(patchwork)
theme_set(theme_classic())
## SET YOUR SEED BELOW!!! Make it YOUR SEED! Uncomment the line, and add your chosen seed number into the parentheses
set.seed(90196) #my birthday
```

## Instructions

Standard grading criteria apply. Make sure you set your seed, and **proofread to submit YOUR OWN WORDS!!**

This assignment will use an external dataset of various physical measurements from 752 adult Pima Native American women, some of whom are type II diabetic and some are not. Our goal for this assignment is to build and evaluate a model from this data to **predict whether an individual has Type II Diabetes** (column `diabetic`).

```{r, collapse=T}
pima <- read_csv("https://raw.githubusercontent.com/sjspielman/datascience_for_biologists/master/data/pima.csv")
dplyr::glimpse(pima)

# Not severely imbalanced. ROC will be fine.
pima %>% 
  count(diabetic) #have a high rate of diabetes. No stepwise model selection or AIC. We are going to build a couple models to see which one we are going to use. Not an equal number of yes and no. 35% are diabetic. It is not imbalanced in a way that we need to be concerned about. If it was severely imbalanced, then we would be concerned. 
```

Columns include:

+ `npreg`: number of times pregnant
+ `glucose`: plasma glucose concentration at 2 hours in an oral glucose tolerance test (units: mg/dL)
+ `dpb`: diastolic blood pressure (units: mm Hg)
+ `skin`: triceps skin-fold thickness (units: mm)
+ `insulin`: 2-hour serum insulin level (units: Î¼U/mL)
+ `bmi`: Body Mass Index
+ `age`: age in years
+ `diabetic`: whether or not the individual has diabetes


## Part 1: Build THREE models


First, you will build THREE distinct models (without stepwise model selection - just build them!) from the full data set.

1. The first model should include *all columns as predictors* (except `diabetic`, of course)


2. The second model should include these two predictors ONLY:
  + `glucose`
  + `bmi`

3. The third model should include these three predictors ONLY (same as #2, but with `skin` added into the mix):
  + `glucose`
  + `bmi`
  + `skin`

**Before you begin**, modify the `diabetic` column so that **Yes** is a "success" and **No** is a "failure." Rather than recoding as 1/0, instead let's just make the column a factor *with levels ordered No, Yes*! This will ensure "Yes" (has diabetes) is a success in the model.


```{r}
## Re-factor the diabetic column
## It is OK to overwrite the original pima dataset and the original diabetic column!!!!!!!!!!!
pima$diabetic <- factor(pima$diabetic, levels = c("No", "Yes")) #success is second! Failure is first!

## Show the output to prove to yourself that the factoring worked
head(pima$diabetic) #worked!
```

```{r}
## Build model 1
model_1 <- glm(diabetic ~ ., data = pima, family = "binomial") #need glm and . means everything

## Build model 2 
model_2 <- glm(diabetic ~ glucose + bmi, data = pima, family = "binomial") #only want glucose and bmi

## Build model 3 
model_3 <- glm(diabetic ~ glucose + bmi + skin, data = pima, family = "binomial") #only want glucose, bmi, and skin
```


## Part 2: Compare the three models from part 1

+ In the space below, determine the AUC associated with each model. 

```{r}
## Run roc() on each fitted model
model_1_roc <- roc(pima$diabetic, model_1$fitted.values)
model_2_roc <- roc(pima$diabetic, model_2$fitted.values)
model_3_roc <- roc(pima$diabetic, model_3$fitted.values)
## Reveal the three models' AUC values here
model_1_roc$auc #never hardcode. You should do. This seems to be the best predictor because the AUC is the highest.
model_2_roc$auc #this model is a bit less. It seems that glucose and bmi are carrying the most weight.
model_3_roc$auc #skin doesn't seem to do very much. It doesn't add much
```


+ Then, plot an ROC curve of all three models in the same panel:

  + Model curves should be distinguished by color
  + Ensure that the models are *ordered* from best to worst in the legend, using AUC to determine this ranking. 
  + Models should be named in the legend as *Model X (AUC=...)*. For example, imagine model 1 has an AUC of 0.7 - it should appear in the legend as *Model 1 (AUC = 0.7)*. HINT: Just put whatever the appropriate phrase is as the name of the model when wrangling!! 
  + **Do not hardcode AUC values.** Always use a VARIABLE. To specifically get the auc value, do something like `roc_output$auc[[1]]` (the two square brackets!).
  
```{r}
## Plot the ROC curves below (which includes some wrangling!!)
#order is 1, 3, 2. 1 is the best and 2 is the worst
tibble(TPR = model_1_roc$sensitivities, 
       FPR = 1 - model_1_roc$specificities,
       model = "Model 1") -> model_1_tibble #TRUE POSITIVE IS SENSITIVITY which goes on the Y AXIS
tibble(TPR = model_2_roc$sensitivities, 
       FPR = 1 - model_2_roc$specificities,
       model = "Model 2") -> model_2_tibble
tibble(TPR = model_3_roc$sensitivities, 
       FPR = 1 - model_3_roc$specificities,
       model = "Model 3") -> model_3_tibble

bind_rows(model_1_tibble, model_2_tibble, model_3_tibble)%>%
  mutate(model_fact = factor(model, 
                             levels = c("Model 1", "Model 3", "Model 2"), 
                             labels = c(paste0("Model 1 (AUC=", round(model_1_roc$auc[[1]],3), ")"), paste0("Model 3 (AUC=", round(model_3_roc$auc[[1]],3), ")"), paste0("Model 2 (AUC=", round(model_2_roc$auc[[1]],3), ")")))) %>% #need to change levels AND labels. Going to change labels to a variable so this doesn't look so gross.
  ggplot(aes(x = FPR, y = TPR, color = model_fact)) +
  geom_line()+
  geom_abline() +
  labs(color = "Models")
```


#### Part 2 Questions
Answer each in 1-2 sentences each *that are CLEARLY WRITTEN in YOUR OWN WORDS without ELEMENTARY-SCHOOL-LEVEL TYPOS.* We! Are! In! College!

1. Which model has the highest AUC value, and what is its AUC? Given that AUC can range 0.5-1, do you believe this is highly accurate model?

    + Model 1 has the highest AUC value with an AUC value of 0.0839. I do believe that this is a highly accurate model becuase it is much closer to 1 than it is to 0.5. I suppose there are better models out there, but I do think that this is a good model.

2. Compare models 2 and 3. What are the AUC values for each? Does including `skin` in the model seem to improve the model's performance?

    + Model 2 and 3 are not very different from each other. The AUC from Model 2 is 0.8186 and the AUC for Model 3 is 0.8198. From here, we can really see that skin adds a very marginal benefit from model 2 to model 3, since this was the only variable added; however, with a 0.0012, I would argue that this does not contribute anything meaningful to the model's performance.

## Part 3: Work with the best model

Determine the best model (highest AUC) from Parts 1 and 2, and use this model for Part 3. Perform the following tasks:

+ Evaluate it with a training and testing split, where *75%* of the data is in the training split. Do NOT hardcode this 75% value!

```{r}
## Training and testing code goes here
train_perc <- 0.75

training_data <- sample_frac(pima, train_perc)
testing_data <- anti_join(pima, training_data)

##training
train_model <- glm(diabetic ~ ., data = training_data, family = "binomial")
train_roc <- roc(training_data$diabetic, train_model$fitted.values)
train_roc$auc
##testing
test_model <- glm(diabetic ~ ., data = testing_data, family = "binomial")
testing_roc <- roc(testing_data$diabetic, test_model$fitted.values)
testing_roc$auc
```


+ Plot the logistic regression curve for the training and testing models you build. This will be two plots, which you should add together with patchwork to display, OR use some wrangling skills to make a faceted plot in the first place. 
  + Do NOT!! color the logistic curve line
  + DO add colored points for individual observations along the curve
  + Rugs are optional
  
```{r}
## Plotting, and associated wrangling for plotting, goes here

## Training plot
tibble(x = train_model$linear.predictors,
       y = train_model$fitted.values,
       diabetic = training_data$diabetic) %>% #need to make sure you put in the right dataset
  ggplot(aes(x = x, y = y)) +
  geom_line()+
  geom_point(aes(color = diabetic), alpha = 0.5, size = 0.4) +
  theme(legend.position = "bottom") +
  ggtitle("Training Results") -> train_plot

#Testing plot
tibble(x = test_model$linear.predictors,
       y = test_model$fitted.values,
       diabetic = testing_data$diabetic) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line()+
  geom_point(aes(color = diabetic), alpha = 0.5, size = 0.4) +
  theme(legend.position = "bottom") +
  ggtitle("Testing Results") -> test_plot

train_plot + test_plot
```  


+ Determine the AUC value for each, and plot the ROC curves for training and testing in the space below. Again, use one plotting panel and distinguish the fits with colors.
```{r}
## ROC for training/testing goes here

#these are the AUC values for each
train_roc$auc
testing_roc$auc

##Need to make into a tibble with specificities and sensitivites to be able to make a plot with them together.

tibble(TPR = train_roc$sensitivities,
       FPR = 1- train_roc$specificities,
       model = "Training") -> training_data

tibble(TPR = testing_roc$sensitivities,
       FPR = 1 - testing_roc$specificities,
       model = "Testing") -> testing_data

#now need to put the two together! Since the columns are the same, you should bind the rows

bind_rows(training_data, testing_data) -> joined_tibbles_test_train

ggplot(joined_tibbles_test_train, aes(x = FPR, y = TPR)) +
  geom_line(aes(color = model)) +
  scale_color_manual(values = c("deepskyblue", "darkorchid")) +
  labs(color = "Model")
```

#### Part 3 Question

How does this model perform when considering a training and testing split? Compare their specific AUC values and discuss whether you believe the model suffers from overfitting or not in 1-2 sentences.

+ When considering both the training and testing data for the model, we can definitely say that the model is not overfitted. We can **confidently** say this because the AUC value for the testing data is actually higher than the training data. If the model was overfitted, we would expect the AUC for the testing data to be signficantly lower than the trainng AUC. The AUC for the testing data was 0.885 and the AUC for the training data was 0.8263. Interestingly, this model performs better on data that it has never seen before than data it has seen before.


## Part 4: Evaluating at a given threshold

Determine the following measures for the best model (the FULL fit from part 1, NOT a training/testing split from part 3!!) assuming a *success threshold of 0.8*. Refer to the slides for formulas!

+ Accuracy
+ False positive rate
+ Positive predictive value

```{r}
threshold <- 0.8
model_1 <- glm(diabetic ~ ., data = pima, family = "binomial")

## Code to calculate goes here
tibble(prob_diabetic = model_1$fitted.values,
       truth = pima$diabetic,
       predict = if_else(prob_diabetic >= threshold, "Yes", "No")) %>%
  mutate(class = case_when(truth == "Yes" & predict == "Yes" ~ "TP",
                           truth == "No" & predict == "No" ~ "TN",
                           truth == "Yes" & predict == "No" ~ "FN",
                           truth == "No" & predict == "Yes" ~ "FP")) %>%
  count(class) %>%
  pivot_wider(names_from = class, values_from = n) -> final_stuf

##Accuracy 0.715 this says that the 80% threshold is prob bad
#ugly way (final_stuff$TP + final_stuff$TN) / (final_stuff$TP + final_stuff$TN + final_stuff$FP + final_stuff$FN)
final_stuf %>%
  mutate((TP +TN)/(TP+TN+FP+FN))

##FP Rate FP/FP+TN Only about 2.45%
final_stuf %>%
  mutate((FP)/(TN+FP))

##Positive predictive value. 83.78%
final_stuf %>%
  mutate((TP)/(TP+FP))
```

## Part 5: Predicting outcomes

Determine the probability that these two new women have diabetes, again using the best model. Here is their data (choose which columns to include in your tibbles based on what's needed for the best model):

+ Female 1
  + `npreg`: 5
  + `glucose`: 110
  + `dpb`: 78
  + `skin`: 20
  + `insulin`: 58
  + `bmi`: 25
  + `age`: 26
+ Female 2
  + `npreg`: 3
  + `glucose`: 175
  + `dpb`: 92
  + `skin`: 28
  + `insulin`: 222
  + `bmi`: 38
  + `age`: 45


```{r}
## code to predict goes here
## make sure to predict PROBABILITIES!!! 
## your code MUST PRINT OUT the probabilities for each woman in the end

tribble(~npreg, ~glucose, ~dbp, ~skin, ~insulin, ~bmi, ~age,
        5, 110, 78, 20, 58, 25, 26,
        3, 175, 92, 28, 222, 38, 45) -> new_pima
predict(model_1, new_pima, type = "response")
```

#### Part 5 Questions:

1. At a threshold of 80%, does the model predict that Woman 1 is diabetic? Answer yes or no!

  + No, at a threshold of 80%, the model **does not** predict that Woman 1 has diabetic. With a threshold of 80%, to get a positive result, you would need to have a result of 80% or above. The first woman had a prediction to have diabetes of 10.64%; therefore, she had a negative result.
  
2. At a threshold of 80%, does the model predict that Woman 2 is diabetic? Answer yes or no!

  + No, the prediction with a threshold of 80% for the second woman is 79.79%. Thus, this is a negative result for diabetes. However, looking at the data, I am not sure this is accurate. This tells us that perhaps the threshold we have chosen is not a good threshold.